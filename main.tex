\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{color}
\usepackage{fancyvrb}
\usepackage{fvextra}
\usepackage{float}
\usepackage[table]{xcolor}
\usepackage{tcolorbox}
\usepackage{listings}
\usepackage{cuted}
\usepackage{xcolor}
\usepackage{appendix}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode} 
\usepackage{mdframed}

\definecolor{lightgray}{rgb}{0.95,0.95,0.95}
\lstset{
    language=Python, 
    basicstyle=\ttfamily\footnotesize,
    breaklines=true, 
    frame=single,
    showstringspaces=false,
    backgroundcolor=\color{lightgray},    
    captionpos=b
}

\begin{document}

%\title{Integrating Generative AI into Mobile Networking}
\title{An LLM Reasoning Framework for Automatic gNB Parameter Adjustment Using Synthetic Training Data}
\author{\IEEEauthorblockN{
        Yao-Cong Dong\IEEEauthorrefmark{1}\IEEEauthorrefmark{4},
        Maria Amparo Canaveras Galdon \IEEEauthorrefmark{2}\IEEEauthorrefmark{3}\IEEEauthorrefmark{4},
        Ray-Guang Cheng\IEEEauthorrefmark{1},
        Januar Evan Zuriel Banjarnahor\IEEEauthorrefmark{1},
        and
        Edwin K. P. Chong\IEEEauthorrefmark{3}
       \\
  }
    \IEEEauthorblockA{\IEEEauthorrefmark{1}
    Dept. of Electronic and Computer Engineering, National Taiwan University of Science and Technology, Taiwan
    }  \IEEEauthorblockA{\IEEEauthorrefmark{2}
    NVDIA 
    }
    \IEEEauthorblockA{\IEEEauthorrefmark{3}
Colorado State University, USA  \\
    }
    \IEEEauthorblockA{\IEEEauthorrefmark{4}
These authors contributed equally to this work.  \\
    }
    Email: crg@mail.ntust.edu.tw, acanaveras@nvidia.com 
    }




\maketitle


% ---------- Abstract ----------


\begin{abstract}
Analyzing and understanding networks for root cause analysis (RCA) is a time-consuming process and a key point to automate on the path toward autonomous networks.
%Analyzing and understanding networks for root cause analysis (RCA) is very time-consuming and a key point to automate in the path for autonomous networks. 
Network logs are text-based and good candidates for leveraging the language-understanding capabilities of large language models (LLMs). 
% Network Logs are text-based and good candidates to leverage the language understanding capabilities of LLMS. 
However, the traditional query and answer (Q\&A) nature of LLMs lacks the human ability to analyze root causes and generate configuration proposals. 
% However, the traditional Q\&A nature of LLMs lacks of capabilities to perform truly root cause analysis and change configuration proposal. 
Recent research in LLM reasoning allows LLMs to emulate human root cause analysis following a series of logical thinking steps.
% Recent research in the LLM reasoning allow llms to follow a series of logical thinking that emulates human root cause analysis. 
In this paper, we propose an LLM reasoning framework for automatically adjusting gNB parameters using synthetic training data. We utilized the proposed framework to automatically generate synthetic training data; train LLMs to think and reason using supervised fine-tuning (SFT); and propose configuration parameters by analyzing network logs. Finally, we utilized open-source Service Management and Orchestration (SMO) and OpenAirInterface (OAI) gNB to verify the feasibility of the proposed approach. 
%In this paper we propose a framework to train LLMs how to think and reason to analyze network logs and propose configurations that will solve the issue



\end{abstract}

\begin{IEEEkeywords}
\end{IEEEkeywords}


\section{Introduction}

% literature review section on synthetic reasoning trace generation
\color{red}{
Johnson: 
Our space is limited, we will not start from LLM. Please focus on essential background knowledge for 'synthetic reasoning trace generation' and remove the less related paragraphs. Please follow the guidelines to rewrite Sec. I. Remove ALL non-essential parts.

Guidelines (confirmed and refined by Amparo):\\
1. The contributions of this paper are:\\
- A reasoning pipeline that maps gNB error logs to reasoning traces for configuration recommendations.\\
- A pipeline to create reasoning traces that can improve the ability of an LLM to interpret gNodeB error logs and think of configuration solutions \\
- A synthetic dataset generation method for fine-tuning LLMs in this domain.\\
-  A verified and human-reviewed synthetic dataset that can be used to fine-tune LLMs\\


2. General idea: Disaggregated 5G gNBs generate complex logs → current rule-based or ML methods are insufficient → LLMs can understand natural-language logs and reason → builds a pipeline to automatically generate the dataset and use the fine-tuned LLM to interpret, and adjust parameters based on reasoning traces.

3. Outline for Introduction (needs to be refined after finish the system model the proposed method):\\

Paragraph 1 - Motivation and context \\
- Disaggregated gNBs require proper setting of configurable parameters in RAN components.\\
- Misconfigurations or dynamic network conditions often lead to error logs and degraded KPIs.\\
- Manual debugging and reconfiguration tuning are slow and error-prone.\\
- Quick conclusion: Autonomous gNB configuration is critical for next-generation RAN operation

{\bf{Restructure the following paragraphs based on Paragraph 1 - Motivation and context}}

\color{blue}{
In the disaggregated 5G gNB architecture, the Central Unit (CU) and Distributed Unit (DU) components must be properly configured to ensure stable network operation. These modules generate event logs \cite{event_log} that record events and messages during system operation, including signaling procedures, parameter changes, error reports, and performance metric data. Event logs provide a complete view of network behavior and serve as a key resource for engineers to diagnose issues and analyze system performance. These logs are large and complex, making manual analysis time-consuming and error-prone\cite{time_consuming_&_prone_to_errors}. In-depth analysis of event logs can detect misconfigurations or abnormal states early, prevent signaling failures and KPI degradation, and guide CU/DU parameter adjustments. Therefore, establishing an efficient and automated event log interpretation capability is critical for the stable operation of next-generation 5G Radio Access Networks (RANs).

The management of the 5G Radio Access Network (RAN) faces a core challenge, stemming primarily from its highly decentralized and modular architecture. While this design significantly enhances the system's flexibility and scalability, it also makes coordination, monitoring, and overall operation and maintenance ($\text{O}\&\text{M}$) across $\text{RAN}$ nodes increasingly complex. As the network environment continuously generates vast amounts of operational logs and configuration data, the critical challenge for advancing the $\text{RAN}$ towards intelligent management is determining how to efficiently and accurately analyze this information to achieve automated fault diagnosis and remediation.

According to the $\text{3GPP}$ TS 38.401\cite{TS138401_NG_RAN}, the architecture of the Next Generation $\text{RAN}$ ($\text{NG-RAN}$) is composed of multiple $\text{gNB}$s. Each $\text{gNB}$ is further logically split into a Central Unit ($\text{CU}$) and a Distributed Unit ($\text{DU}$), which cooperate via the $\text{F}1$ interface. The $\text{gNB}$s interconnect via the $\text{Xn}$ interface and connect to the $5\text{G}$ Core Network ($\text{5GC}$) via the $\text{NG}$ interface. The core operational behavior of the $\text{gNB}$ within the $\text{NG-RAN}$ is driven by a set of configuration files, which define key operational logic, including power control, bandwidth allocation, antenna settings, and protocol parameters. During system operation, the equipment continuously generates large volumes of log data, detailing various interactions, performance metrics, and error codes for $\text{CU}–\text{DU}$ ($\text{F}1$), $\text{CU}–\text{CN}$ ($\text{NG}$), and $\text{DU}–\text{RU}/\text{UE}$, thus forming a complete and traceable operational record. However, while this multi-layered, open architecture design significantly enhances network flexibility and modularity, it also makes configuration management and troubleshooting work more complex and difficult, with challenges being particularly pronounced in multi-vendor and heterogeneous network environments.

The traditional Operations and Maintenance ($\text{O}\&\text{M}$) workflow ( as shown in Fig.~\ref{model}) is a manual-based closed-loop process: First, human experts (or the expert system) acquire system data from the $\text{RAN}$ environment, such as configuration files, execution logs, and alarm notifications (Step 1); next, the expert system analyzes the data based on personal experience, professional knowledge, and internal documentation to determine the cause of the fault (Step 2), and formulates an adjustment strategy (e.g., configuration adjustments based on $\text{SLA}$ or vendor policies) (Step 3); finally, these adjustment commands or new configuration files are redeployed into the $\text{RAN}$, and the process continuously monitors and repeats these three steps. This approach is not only time-consuming, subjective, and difficult to standardize, but also highly reliant on the experience of individual experts, which easily leads to knowledge loss and inconsistent fault handling when personnel turnover occurs or records are incomplete \cite{llm4telecom_survey}.

}


\color{red}{
Paragraph 2 — Problem Statement and Research Gap\\
- Traditional SON (Self-Organizing Networks) and rule-based systems rely on predefined logic, lacking adaptability to unseen issues.\\
- ML-based anomaly detection or KPI optimization methods often require labeled data and structured features, not raw log text.\\
- Error logs are mostly unstructured, requiring natural-language understanding and reasoning.\\
- Existing network automation methods cannot reason from textual error logs to actionable configuration adjustments.\\
- LLMs may bridge the gap between unstructured log analysis and configurable network control.\\
}
\color{blue}{
However, despite the critical role of event logs in diagnosing and maintaining 5G networks, existing automation approaches remain limited in their ability to interpret and act upon such complex log data. Traditional Self-Organizing Networks (SON) and rule-based systems depend on predefined logic, which can only address known fault patterns but fail to adapt to novel or cross-module issues\cite{nadella2025evolution}. Machine learning-based methods for anomaly detection and KPI optimization often require labeled datasets and structured features\cite{Large_quantities_of_annotated_data}, making them unsuitable for processing raw, unstructured event logs. In practice, gNB logs contain heterogeneous and context-dependent information—such as signaling traces, timing errors, module interactions, and performance indicators—that demand semantic understanding and causal reasoning to interpret effectively. Current network automation frameworks lack this reasoning capability, preventing them from deriving actionable configuration adjustments directly from log text. As 5G networks evolve toward dynamic, multi-vendor, and software-defined environments, this limitation increasingly constrains autonomous network management. Large Language Models (LLMs) provide a promising alternative: they can comprehend unstructured text, infer causal relationships, and generate meaningful recommendations. Leveraging LLMs to interpret event logs and propose configuration corrections thus represents a crucial step toward achieving fully autonomous and reliable network operation.
}

\color{red}{
Paragraph 3 — LLM Potential for Autonomous Network Reasoning \\
Purpose: Position LLMs as the key enabler for reasoning-based automation. \\
Key ideas:\\
- LLMs can interpret natural-language logs, reason causally, and propose configuration corrections.\\
- Recent works show LLMs can be fine-tuned or prompt-engineered to interpret structured or semi-structured telemetry data.\\
- However, direct application to gNB log-based control is underexplored. \\
- This work proposes a system leveraging LLM-generated reasoning traces to autonomously recommend gNB configuration adjustments.
- This work is different because we leverage the research in reasoning, which has been widely applied to math and code problems, and apply it to network log interpretation and resolution with proper configuration.\\
}

\color{blue}{Large Language Models (LLMs) possess the capability to understand unstructured text and perform causal reasoning, offering new opportunities for automated network management. They can analyze error messages and system behaviors in event logs, identify potential root causes, and propose actionable parameter adjustments. Recent studies have shown that LLMs can be fine-tuned or guided through prompt engineering to process semi-structured or structured telemetry data. However, direct application of these models to gNodeB log analysis and network control remains underexplored. The approach proposed in this paper leverages LLM-generated reasoning traces to transform event logs into structured reasoning processes, enabling automatic generation of configuration recommendations. Unlike existing methods, this work applies techniques from mathematical and code reasoning to network log analysis, providing causal reasoning and actionable suggestions. This approach improves adaptability to unseen errors, reduces human intervention, and offers reliable automation support for dynamic, multi-vendor 5G RAN operations.}

\color{red}{

Paragraph 4 — Proposed Approach and Contributions \\
This paper proposes an LLM-driven gNB configuration reasoning framework that interprets error logs, infers root causes, and suggests parameter adjustments via synthetic reasoning trace generation. \\

Contributions:\\
- A log-to-reasoning pipeline that translates gNB error logs into structured reasoning traces.\\
- Utilize AI coding assistants to automatically generate a synthetic reasoning dataset.\\
- Training SFT pipeline leveraging the reasoning traces to instill new knowledge in LLM.\\
- An LLM inference loop that recommends configuration changes and validates them using feedback logs.\\
- Evaluation using real gNB logs, demonstrating improved interpretability and configuration success rate. And demonstrating increased Network Troubleshooting generalization capabilities in the LLM.
}


\color{blue}{
% This paper proposes an LLM-driven gNB configuration reasoning framework that automatically interprets event logs, infers root causes, and provides actionable recommendations for parameter adjustments. The framework first converts error event logs into structured reasoning traces, capturing causal relationships and decision-making processes. It leverages AI coding assistants to automatically generate a high-quality synthetic reasoning dataset. This dataset is used for supervised fine-tuning (SFT) of the LLM to enhance its understanding of network logs and configuration capabilities. Finally, the framework applies the LLM-recommended parameter adjustments to the network and validates them through subsequent event logs in a reasoning loop.  
}

\color{black}{
This paper presents an LLM reasoning framework for automatic gNB parameter adjustment. In this framework, we first convert error event logs from gNB into structured reasoning traces to capture causal relationships and decision-making processes. We leverage AI coding assistants to automatically generate a synthetic reasoning dataset and use supervised fine-tuning (SFT) to enhance LLM's understanding of network logs and configuration capabilities. 
The main contributions of this paper include:
\begin{itemize}
\item Developing a log-to-reasoning pipeline that translates gNB error logs into structured reasoning traces.
\item Utilizing AI coding assistants to automatically generate a synthetic reasoning dataset.
\item Proposing a supervised fine-tuning (SFT) training pipeline that leverages the reasoning traces to instill new knowledge in LLM.
\item Implementing an LLM inference loop that recommends configuration changes and validates the configuration using feedback logs from gNB.
\end{itemize}

The rest of this paper is organized as follows. Section~\ref{SystemModel} defines the system model considered in this paper. The proposed LLM reasoning framework is given in Section~\ref{LLMReasoningFramework}.
We use OpenAirInterface (OAI) gNB to demonstrate the pipeline required to implement the proposed LLM reasoning framework. Section~\ref{ExperimentalResults} presents the experimental results. 
Experimental results demonstrate that the proposed approach enhances the interpretability of LLM outputs for gNB logs, increases configuration success rates, and significantly enhances the generalization ability in network troubleshooting.
Section~\ref{Conclusions} provides the concluding remarks.
}

\section{System Model}\label{SystemModel}

Figure~\ref{model} shows a high-level architecture of an O-RAN management and control system considered in this paper. The system comprises three main components: an operator, a management platform, and an open and disaggregated RAN. 
The operator could be a human, a machine, or an AI model that has RAN-specific domain knowledge of terminology and log semantics to interact with the network and perform root cause analysis. The operator communicates with the underlying management platform through the operator interface or management application interfaces (APIs), \textcolor{blue}{and It can be divided into the following three stages:}
\textcolor{blue}{
\begin{itemize} 
    \item Data Collection - The operator uses operator interface or APIs to obtain relevant RAN data from the management platform, such as configuration files and logs.
    \item Analysis - The operator utilizes the acquired data to perform further analysis and decision-making
    \item Execution - The operator communicates with the management platform through operator interface or APIs to translate analysis decisions into actual network operations, such as submitting new configuration recommendations, for the purpose of controlling and monitoring the RAN.
\end{itemize}
}

\begin{figure}[h]
\centering
\includegraphics[width=1\linewidth]{figure/LLM-system_model_version_v6.drawio.png}
  \caption{System model}
  \label{model}
\end{figure}

The management platform could be an Operations Support System (OSS) used in traditional telecom networks or a Service Management and Orchestration (SMO) defined by the O-RAN ALLIANCE. The management platform utilizes the standard open interface (e.g., O1 interface) following the standard YANG model defined by 3GPP \cite{} to configure the O-RAN network functions. 
With the standard open interface, error logs or degraded key performance metrics (KPIs) resulting from misconfigurations of the O-RAN network functions can be automatically reported to the management platform.

Without loss of generality, we consider the simple case in which the operator analyzes error logs and recommends network configurations for the O-RAN network functions. In this paper, we use the following metrics \cite{} to evaluate the performance of the operator by considering both linguistic accuracy and operational results of the network.


\color{red}{
List the metrics and their definitions:
}


\color{black}
\section{LLM Reasoning Framework}\label{LLMReasoningFramework}
This paper presents an LLM reasoning framework 
that establishes a pipeline from error logs to reasoning traces for automatic gNB parameter adjustment. \textcolor{blue}{Fig.~\ref{LLM_reasoning_framework} illustrates the proposed framework, comprising two main components: synthetic data generation and supervised fine-tuning (SFT)}
The LLM implements a reasoning loop that provides reliable configuration recommendations and validation. We expect the LLM can automatically interpret event logs collected from gNB, infer root causes, and provide actionable recommendations for parameter adjustments. 
Synthetic data generation utilizes a reasoning loop to automatically generate a high-quality synthetic reasoning dataset by converting error event logs into structured reasoning traces and capturing causal relationships and decision-making processes. We utilize AI coding assistants to generate invalid configuration parameters and validate the root causes of these incorrect parameters through subsequent event logs collected from the RAN. We then develop an SFT pipeline to instill the domain-specific knowledge of network logs and gNB configurations obtained from the synthetic reasoning dataset into the LLM.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figure/LLM-LLM Reasoning Framework_v3.drawio.png}
    \caption{LLM Reasoning Framework}
    \label{LLM_reasoning_framework}
\end{figure}

\color{red}{
[Ray:]
1. ALL the blocks shown on the figure should be explained. On Fig. 1, you don't have to show the internal blocks of the Management Platform unless needed. {\bf Similarly, we should not draw the internal implementation of the Operator.}

[Johnson:] Done

2. Please use the {\bf same term you used in the figure} to revise the text shown above. The terms of Error Log, Error conf, Base LLM, Fine-tuned LLM should be mentioned in the text.
{\bf Similarly, the same terms should be used in ALL figures.}

% [Johnson:] Done, Base LLM and Fine-tuned LLM will mentioned in Supervised Fine Tuning
% %%
3. Do you input Error Conf to AI Coding Assistant? Or, use AI Coding Assistant to generate Error Conf? {\bf Use your block diagram to show us YOUR implementation. Your statement is inconsistent with Fig. 2.}

% [Johnson] I input Workable configuration and use the AI Coding Assistant to mutate it into an Error Configuration through a Configuration Mutation Prompt.
% %%

4. Remove LLM from the AI Coding Assistant. It's NOT the same as the LLM you showed in the Supervised Fine-tuning block. {\bf How many LLM do you use in this framework? Are the Reasoning LLM and Base LLM the same?}

% [Johnson:] Done
% %%
5. How do you link your Figs. 1 to 2?
6. Please refer to the other paper to draw your figures. Figs. 1 and 2 are NOT consistent.
% [Johnson]: I will remove the original Figs. 3
% % %%
% [Johnson]: I will remove the original Figs. 3 and 4
% %%
% 7. From Fig. 1, the Operator DIDN't connect to RAN. Why do you show RAN on Fig. 5?
% [Johnson]: I will remove the original Figs. 5
%%
7. In Appendix A, is it 'CONFIGURATION MUTATION PROMPT' or 'Reasoning Trace Prompt.'

8. Please use the SAME term in the text, figure, and APPENDIX A. You should be able to identify the inconsistencies among figures before writing them down. Your figure should be based on your implementation. {\bf Add a table to list the codes you implement the claimed functions on Fig. 2. Show us the I/Ps and O/Ps of each block.}

You have different versions for the I/Ps and O/Ps for AI Coding Assistant (from your answers to 3 and 8):

[Johnson]: I input Workable configuration and use the AI Coding Assistant to mutate it into an Error Configuration through a Configuration Mutation Prompt.

[Johnson]: I/P to AI Coding Assistant is Error Log and Error Configuration files (Error Conf). O/P is Reasoning Trace.

[Johnson]: I/P to AI Coding Assistant becomes Configuration Mutation Prompt and Workable Configuration Files. O/P is Configuration Mutation Set.

[APPENDIX A]: Input: Workable .conf file, Reference .json file, Output Schema? 

% - I/P to AI Coding Assistant is Error Log and Error Configuration files (Error Conf). 

% - O/P is Reasoning Trace

% On Fig. 4, 

% - I/P to AI Coding Assistant becomes Configuration Mutation Prompt and Workable Configuration Files.

% - O/P is Configuration Mutation Set

% [Johnson]: I already remove Fig. 4 and confirmed Figure 2 with Amparo.


% For APPENDIX A:

9. How does your prompt accurately generate synthetic data to represent rare or unusual events? {\bf How do you plan to prove it?}
% [Johnson]: Just offering a few directions for the AI Coding Assistant to consider.

10. Is the 'COMPONENT' in APPENDIX A the same as the 'Component' in Table II? 

[Johnson]: Only CU/DU/UE

{\bf The Management Platform (SMO/OSS) can only get data from RAN. How can your Management Platform configure and get the logs from UE? From your Reasoning Trace Prompt, 'You are a 5G gNB configuration fuzz-test expert.'}

11. What $N_VARIATIONS$ did you use? How can you ensure the AI Coding Assistant can produce distinct, unique test cases? 

[Johnson] I am currently using 25 as the quantity for $N_{VARIATIONS}$.

[Johnson] To ensure the AI Coding Assistant generates unique and diverse test cases, the LLM temperature is set to 1.This encourages stochastic sampling, allowing the model to explore multiple valid configurations instead of deterministic outputs.

[ChatGPT]  
{\bf Temperature = 1 does not guarantee uniqueness or distinctness, and relying on it to ensure unique gNB error configurations is not technically valid. \\
Your student’s claim:
“…temperature=1 ensures the AI assistant produces distinct, unique test cases…”
is incorrect. Temperature has no guarantee of uniqueness. Real uniqueness requires explicit constraint enforcement, deduplication, or hybrid generation logic.}

12. How do you ensure the AI Coding Assistant can accurately determine the error categories (e.g., the valid range of a parameter)? {\bf Please answer!}

13. How/Where do you design Log Analysis and Reasoning? {\bf Please answer!}

14. (Copied from Line 397) Please refer to Appendix when you want to mention Configuration Mutation Prompt and Reasoning Trace "Generation??" Prompt?

15. How do you generate 400 test cases using the prompt of "Produce {\bf{25}} distinct cases, covering different error categories."?

16. On Line 407, you mentioned \\
This three-module architecture forms a continuous and automated pipeline that bridges configuration manipulation, real network execution, and reasoning-based log interpretation. It not only ensures data consistency and technical fidelity but also allows scalable generation of high-quality reasoning traces for fine-tuning.

{\bf How can you ensure data consistency, technical fidelity, and scalable generation of high-quality reasoning traces for fine-tuning?}

}
\color{blue}{
\subsection{Key Framework Parameters}
\begin{itemize} 
    \item \textbf{Workable Configuration ($C_{\text{work}}$):} A configuration file enabling the gNB to work properly.

       
    \item \textbf{Configuration Mutation Prompt ($P_{\text{mut}}$):} A prompt designed to guide the AI coding assistant generates Modified Configurations based on the input Workable Configuration (see Appendix~\ref{app:gen_prompt} for details).

    
    \item \textbf{AI Coding Assistant ($AI_{\text{code}}$):} An agent or AI-assisted tool driven by the $P_{\text{mut}}$, which randomly modifies parameters of a $C_{\text{work}}$ to generate new configurations.
    
    \item \textbf{Modified Configurations ($C_{\text{mod}}$):} A set of configuration files generated by the AI Coding Assistant based on Workable Configurations

    \item \textbf{Error Logs ($L_\text{error}$):} The logs containing error messages or missing key information, generated during the operation of the gNB under $C_{\text{mod}}$.

    \item \textbf{Reasoning Trace Prompt ($P_{\text{trace}}$):} A prompt designed to define the $\text{LLM}_{\text{reason}}$'s role, tone, reasoning steps, and output structure.
    \item \textbf{Reasoning LLM ($LLM_{\text{reason}}$):} An LLM designed to execute $P_{\text{trace}}$ and generate structured reasoning traces.
        

    \item \textbf{SFT Training Data ($D_{\text{SFT}}$):} The output from the $LLM_{Reason}$, records the structured reasoning trace for each input $L_\text{error}$ and $\text{C}_{\text{work}}$, and adjustment suggestions.
    
    \item \textbf{Base LLM ($LLM_{\text{base}}$):}
    \item \textbf{Fine-turned LLM ($LLM_{\text{sft}}$):}


    
\end{itemize}


}

\subsection{Synthetic Data Generation}
\textcolor{red}{
1. To achieve "this" objective: What is "this"?
2. In your original text, you claimed:
``Fig. 2 illustrates the proposed framework, comprising three main components:
synthetic data generation, supervised fine-tuning (SFT), and LLM.'' Please show the three components and the interrelationship of the three components on your figure. 
}

[Johnson] The original document "Figure 2 illustrates the proposed architecture, which includes three main components: synthetic data generation, supervised fine-tuning (SFT), and LLM" has been updated to "Figure 2 illustrates the proposed architecture, which includes two main components: synthetic data generation and supervised fine-tuning (SFT)".

\color{blue}{
The purpose of the Synthetic Data Generation component is to construct training datasets for SFT. We adopt a three-stage process as illustrated in Figure 2 under Synthetic Data Generation:
}

\subsubsection{Configuration Generation and Modification}

\color{blue}{
This stage is responsible for generating a large number of configuration files with different variations based on input configuration files. Once the $\text{C}_{\text{work}}$ and $P_{\text{mut}}$ are received, the $AI_{\text{code}}$ will generate the $C_{\text{mod}}$ according to the following steps:

\begin{itemize}
  \item Analyze Input Files - $AI_{\text{code}}$ first analyzes the $\text{C}_{\text{work}}$ and determines the parameters that can be modified and their types.
  \item Generate CMS - $AI_{\text{code}}$ generates a config mutation set (CMS) based on the understanding in Analyze Input Files step. The key structure is shown in Table \ref{table:Definition_of_Config_Mutation_Set} , and the format is shown in output schema in the appendix \ref{app:gen_prompt}.
  \item Generate Modified Configurations - Based on the modified\_key and error\_value in the CMS generated in Generate CMS, modify the $C_{\text{work}}$ to generate $C_{\text{mod}}$.
\end{itemize}

To ensure that the model understands the task objectives and response logic precisely, prior work \cite{network_configuration_cn} provides an architecture that structure prompt messages into four core elements: role, task description, background context, and expected behavior. Table \ref{tab:prompt_configuration_mutation} presents the prompt architecture adopted in this study in Reasoning Trace.}


\begin{table}[H]
    \centering
    \caption{Definition of Config Mutation Set}
    \label{table:Definition_of_Config_Mutation_Set}
    \begin{tabular}{|m{2.5cm}|m{5.5cm}|}
        \hline
        \textbf{Component} & \textbf{Definition and Purpose} \\
        \hline
        \texttt{filename} & Specifies the name of the base configuration file to which the modification should be applied. \\
        \hline
        \texttt{modified\_key} & Defines the hierarchical path (using JSON Path notation) to the specific parameter that needs to be altered. \\
        \hline
        \texttt{original\_value} & Records the value of the parameter as found in the initial, workable configuration file for verification. \\
        \hline
        \texttt{error\_value} & The new value that replaces the original value. This often represents an injected fault, a boundary case, or a required new setting. \\
        \hline
    \end{tabular}
\end{table}

\begin{table}[H]
    \centering
    \caption{Components of Configuration Mutation Prompt}
    \label{tab:prompt_configuration_mutation}
    \renewcommand{\arraystretch}{1.5} % 調整行高，讓文字不會貼著邊框
    \begin{tabular}{|m{1.5cm}|m{6.5cm}|}
        \hline
        \textbf{Prompt Component} & \textbf{Implementation} \\
        \hline
        
        Role & 
        ``You are a 5G gNB configuration fuzz-test expert specializing in the \texttt{\{COMPONENT\}} component.'' \\
        \hline
        
        Task \newline Description & 
        ``Generate exactly \texttt{\{N\_VARIATIONS\}} distinct, single-key error test cases based on the provided valid reference configuration. Modify exactly one key per case.'' \\
        \hline
        
        Background Context & 
        \textbf{Input Data:} Valid \texttt{.conf} and \texttt{.json} configuration files (Baseline \& Reference). \newline
        \textbf{Domain Constraints:} Defined error categories (e.g., out-of-range, invalid enum, logical contradiction, wrong type).
        \\
        \hline
        
        Expected Behaviour & 
        ``Output MUST be a pure JSON array following the strict Output Schema. Do not include any conversational text. Ensure errors are realistic and likely to cause system faults.'' \\
        \hline
    \end{tabular}
\end{table}


\subsubsection{Deployment and Log Processing}
\textcolor{blue}{This stage is responsible for deploying the $C_{\text{mod}}$ generated during the Configuration Generation and Modification stage into the RAN via the management platform interface, executing them, and collecting logs generated by all components within the RAN.}

\color{blue}{Once the Management Platform receives $C_{\text{mod}}$, it will get $L_\text{error}$ according to the following steps:

\begin{itemize}
  \item Configuration Deployment - The Operator deploys $C_{\text{mod}}$ to the RAN via the Management Platform.
  \item Log Transmission - The RAN returns the collected logs to the Management Platform via standard interfaces (e.g., O1) or other connection methods (e.g., API) after completing the configuration execution.
  \item $L_\text{error}$ Selection - The management platform receives logs from the RAN and removes logs that do not have error features. The error feature definitions used in this paper will be detailed in the \ref{ExperimentalResults} section.
\end{itemize}
}


\subsubsection{Log Analysis and Reasoning}
\textcolor{blue}{This stage is responsible for converting the $C_\text{mod}$ and $L_\text{error}$ generated into Reasoning Traces throughput the $LLM_{Reason}$ which is the training data used for SFT in this paper. Table \ref{tab:prompt_reasoning_trace} presents the prompt architecture adopted in this study in Reasoning Trace.}


We used multiple prompt engineering methods to guide $LLM_{Reason}$. First, we used role-playing prompting \cite{tseng2024two} to explicitly define the $LLM_{Reason}$ as an “expert 5G NR and OpenAir Interface (OAI) network analyst”. Also, we ask the model to reason in the first person, using statements such as ‘I begin by...’, ‘I observe that...’, and ‘I hypothesize that...’, to imitate the reasoning tone and analysis style of a real engineer. Second, to analyze complex 5G gNB issues more effectively, we use the Chain-of-Thought (CoT) prompting skill \cite{wei2022chain}, instructing the $LLM_{Reason}$ to generate a sequence of reasoning steps. 

In this paper, we structure the whole reasoning process into five clear steps to build a standardized multi-step reasoning template. These sub-tasks include: Initial Observations, Exploratory Analysis, Log and Configuration Correlation, Root Cause Hypothesis, and Summary and Configuration Fix.



\begin{table}[H]
    \centering
    \caption{Components of Reasoning Trace Prompt}
    \label{tab:prompt_reasoning_trace}
    \renewcommand{\arraystretch}{1.5} % 調整行高，讓文字不會貼著邊框
    \begin{tabular}{|m{1.5cm}|m{6.5cm}|}
        \hline
        \textbf{Prompt Component} & \textbf{Implementation} \\
        \hline
        
        Role & 
        ``You are an expert 5G NR and OpenAirInterface (OAI) network analyst with a talent for creative and thorough problem-solving.'' \\
        \hline
        
        Task Description & 
        ``Your task is to analyze network issues and deduce the precise root cause the exact \texttt{misconfigured\_param} and its wrong value through the strongest possible logical reasoning based on logs and configuration.'' \\
        \hline
        
        Background Context & 
        \textbf{Input Data:} Logs, \texttt{network\_config}, and \texttt{misconfigured\_param}. \newline
        \textbf{Constraint:} Base entire analysis ONLY on logs and network configuration. DO NOT mention the \texttt{misconfigured\_param} until the final conclusion. \\
        \hline
        
        Expected Behaviour & 
        ``Structure your response as a reasoning trace with clearly labeled sections:
        \begin{enumerate}
            \item \textbf{Initial Observations:} Summarize key elements of logs/config.
            \item \textbf{Exploratory Analysis:} Analyze data steps and form hypotheses.
            \item \textbf{Log and Configuration Correlation:} Connect logs to config settings.
            \item \textbf{Root Cause Hypothesis:} Propose the exact parameter and value.
            \item \textbf{Summary and Configuration Fix:} Provide the fix in JSON format.''
        \end{enumerate}
        \\
        \hline
    \end{tabular}
\end{table}





% Third, our prompts explicitly require the Reasoning LLM to “Base your entire analysis ONLY on the logs and \texttt{network\_config},”  and “Every hypothesis…must be explicitly justified with direct references to specific log entries and configuration lines.” This ensures every step of the model's reasoning is based on verifiable evidence, preventing hallucinations and enabling researchers to clearly validate the correctness of supporting logic and causal relationships. Finally, to ensure the Reasoning LLM generates outputs more aligned with our expected outcomes, we included a complete example in the prompt. This example instructs the Reasoning LLM on how to perform reasoning, what style to use for descriptions, and how to reference logs/config files. This approach helps ensure quality of model outputs and reduces formatting inconsistencies.


\subsection{Supervised Fine-tuning}


% \section{Temp}
% {\color{red} Paragraphs put in the wrong sections will be moved here!}



% This research proposes a synthetic data generation workflow, as illustrated in Figure~\ref{Synthetic_data_generation_workflow}. 

% {\color{red} 
% Where is your AI coding Agent on Fig. 3? 

% Please refer to Appendix when you want to mention Configuration Mutation Prompt and Reasoning Trace "Generation??" Prompt?

% How do you generate 400 test cases using the prompt of "Produce {\bf{25}} distinct cases, covering different error categories."?

% On Fig. 3, you should remove everything for the RAN since the SMO is the only interface from Operator's perspective!
% }

% This three-module architecture forms a continuous and automated pipeline that bridges configuration manipulation, real network execution, and reasoning-based log interpretation. It not only ensures data consistency and technical fidelity but also allows scalable generation of high-quality reasoning traces for fine-tuning.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=1\linewidth]{figure/LLM-overview_v1.drawio.png}
%     \caption{Synthetic data generation workflow}
%     \label{Synthetic_data_generation_workflow}
% \end{figure}

% \color{blue}{
% In the overall process, the system first (Block 1) utilizes the LLM to automatically generate new and appropriately {\color{red}(definition is required!)} modified 5G Configurations based on an input prompt and a workable {\color{red}(definition is required!)} configuration file. Next (Block 2), these {\color{red}(what does "these" mean?)} configurations are deployed to the OAI $5\text{G}$ test platform, which includes components such as the OAI CN, OAI CU, OAI DU, and OAI UE, {\color{red}(Did you deploy configurations to all of them?)} for actual execution and functional verification. Finally (Block 3), the system collects the OAI operational logs {\color{red}(what is "operational log" mean?)}, which are then analyzed by the LLM to generate "Reasoning Traces". These reasoning traces will be organized into high-quality training data {\color{red}(HOW?)} for subsequent SFT of the LLM, enabling the model to gradually acquire the ability to understand and infer $5\text{G}$ network behavior, thereby improving its performance in the field of network automation.
% }





% ---------- Error Log Generation Architecture ----------
% \subsection{Synthetic Data Generation}

% Figure $\ref{Error_Log_Generation_Architecture}$ illustrates the error log generation architecture proposed in this study. 
% {\color{red}(Please describe ALL components shown on your figure!)}
% The primary objective of this system 
% {\color{red}(What is the "system"?)} 
% is to construct an efficient and controllable {\color{red}(How do you construct "efficient and controllable"?)} 
% experimental platform for systematically synthesizing fault data within the 5G 
% {\color{red}(Should we limit our work to 5G?)} 
% network environment. The architecture integrates the flexibility of $\text{LLMS}$ 
% {\color{red}(What is this?)} 
% in configuration mutation with the $\text{E2E}$ simulation capability of $\text{OAI}$, enabling automated generation 
% {\color{red}(Please write down HOW you "enable automated generation".)} 
% of high-quality test logs 
% {\color{red}(Please prove that how you generate "high-quality test logs".)} 
% with clearly defined root causes. 
% {\color{red}(Please show us how you get "clearly defined root causes".)} 
% This design not only replicates potential fault scenarios found in real-world network operations but also establishes a traceable and semantically aligned dataset for subsequent $\text{SFT}$ model training.
% {\color{red}(How can you prove it?)} 

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=1\linewidth]{figure/error_log_gen.png}
%     \caption{Error Log Generation Architecture}
%     \label{Error_Log_Generation_Architecture}
% \end{figure}

% The overall workflow 
% {\color{red}(of What?)} 
% consists of three main stages. 
% {\color{red}(Where are the "three main stages"?)} 
% First, the system starts from a verified Workable Configuration File that ensures stable $\text{E2E}$ connectivity for the $\text{UE}$ within the $\text{OAI}$ environment, serving as a reliable experimental baseline. Second, a configuration mutation prompt is designed to guide the LLM 
% {\color{red}(We have AI coding assistant and LLM. Don't mix them up!)}
% in performing controlled variations, specifying the target components (e.g., CU, DU, UE), mutation strategies (e.g., parameter out-of-range, logical inconsistency, or syntax errors), and the desired diversity of generated samples. This allows the model to produce multiple Modified Configuration Files containing potential faults in a systematic and scalable manner. Third, the generated configurations are automatically deployed into the OAI E2E testbed—comprising the Core Network, CU, DU, and UE—and executed in batches through the RF Simulator. This process triggers a variety of fault scenarios such as gNB startup failure, cell search errors, random access issues, radio resource control (RRC) establishment failure, or NG interface handshake problems. Throughout each run, detailed protocol-level logs from CU, DU, and UE are continuously collected to capture precise timestamps, failure locations, and protocol symptoms. The resulting configuration–log paired dataset establishes explicit causal relationships between known configuration changes and observed system faults, forming a critical foundation for intelligent reasoning and supervised fine-tuning in subsequent model training.



% ----------  Reasoning-Trace Generation Architecture ----------
% \subsection{Reasoning Trace Extraction}


% After completion the automated generation of high-quality fault data, this study further proposes the reasoning trace generation architecture, as illustrated in Figure $\ref{Reasoning_Trace_Generation_Architecture}$. 
% {\color{red}(Please describe ALL components shown on your figure!)}
% The goal of this module is to automatically extract causal relationships and semantic representations from fault logs and their corresponding configuration files, generating coherent and interpretable reasoning traces that support knowledge construction and model training for automated debugging.

% \begin{figure}[h]
% \centering
% \includegraphics[width=0.9\linewidth]{figure/error_log_analyze.png}
%   \caption{Reasoning-Trace Generation Architecture.}
%   \label{Reasoning_Trace_Generation_Architecture}
% \end{figure}

% The system aligns and formats the CU, DU, and UE logs 
% {\color{red}Should we add the prompt for UE?} 
% along with their corresponding Modified Configuration Files from the OAI environment into a unified Defined Input Format (as shown in Table \ref{tab:DefinedInputFormat}). This format retains key elements related to root causes, including abnormal parameters {\color{red}(?)}, protocol error codes {\color{red}(?)}, and event sequences {\color{red}(?)}, providing the model with structured and semantically meaningful input data {\color{red}(?)}. 
% {\color{red}Please use the table to show us ho you do it! How do you choose the log?}

% Then, a Reasoning Trace Prompt (see Appendix \ref{app:reasoning_trace_prompt}) is designed to guide the LLM in generating reasoning traces for each fault case. The prompt template explicitly defines the output structure and logical hierarchy, allowing the model to consistently describe the complete causal chain from symptom observation to root-cause inference.
% {\color{red}Could we briefly summarize the design concept of the prompt? What is the main differences between DU and CU Config.?}

% Finally, the generated reasoning traces are systematically collected and organized as SFT training data, serving as high-quality exemplars that teach the model how to reason through fault diagnosis tasks in a structured and explainable manner.




\section{Experimental Results}\label{ExperimentalResults}

In this paper, we use Cursor \cite{cursor2025} and GitHub Copilot \cite{copilot2025} to build the pipeline and create error logs and reasoning traces. 
OAI [] is used as the RAN test environment, and the components in the RAN environment are defined as follows.


We also use OAI \cite{} as the RAN test environment and deploy O-RAN Software Community (OSC) SMO \cite{} as the management and control platform. The experimental architecture is shown in the figure 

\ref{Experimental_Architecture}.

\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{figure/Experimental Architecture.png}
    \caption{Experimental Architecture}
    \label{Experimental_Architecture}
\end{figure*}

\begin{table}[H]
    \centering
    \caption{Definition of RAN Environment Component}
    \label{table:oai_testing_environment}
    \begin{tabular}{|m{1.5cm}|m{6.5cm}|}
        \hline
        \textbf{Component} & \textbf{Definition and Purpose} \\
        \hline
        OAI CN & Contains multiple 5G core function elements, such as AMF, SMF, UPF, primarily responsible for user session management and data plane traffic processing. \\
        \hline
        OAI CU & Handles the RRC, SDAP, and PDCP functions within the 5G gNB protocols. \\
        \hline
        OAI DU & Handles the RLC and MAC functions of the 5G gNB, and features an integrated RF Simulator for emulating the radio environment. \\
        \hline
        OAI UE & Can establish connection and exchange data with the OAI network, used to simulate end-to-end connection scenarios. \\
        \hline
    \end{tabular}
\end{table}













\section{Conclusions}\label{Conclusions}



% ---------- References (BibTeX) ----------
\bibliographystyle{IEEEtran}
\bibliography{reference}



% ---------- appendix----------
\onecolumn
\clearpage 
\appendices        
\input{appendix}


\end{document}
